---
title:  CNN 알고리즘 (1) LeNet  정리
tags:
  - Study
  - ResNet
  - Network
  - Paper
---

LeNet

CNN을 개발한 연구팀이 1998년에 "Gradient-based learning applied to document recognition" 으로 발견함.

<!--more-->

본논문 요약



GTN(Graph Transformer Networks) 에서 특히 Gradient 기반 학습을 하게 한다. 

CNN을 통해 직접 feature selection 을 할 필요가 없게됨

GTN을 통해 parameter tuning, labeling, experience에 기반한 방법의 사용을 줄일 수 있었다.

데이터가 많아지고 컴퓨팅 파워 향샹, 학습 알고리즘의 향상이 인식 시스템보다 더 좋은 향상을 가능하게 했다.



traditional 한 pattern 인식 model은 연관이 없는 정보를 제거하는 방식으로 가기 때문에 이미지사이즈가 커지고, weight parameter 도 커져서 translation(문자 위치 이동과 회전), distortion(문자 형태 변형)을 고려하기 어려움이 있었다. 



CNN에서 크게 

Local receptive field, Shared weights, sub-sampling 으로 architecture 가 구성된다.



receptive filed는 특정 neuron 이 반응하는 것이 모티브이다. 보통 특정사이즈의 kernel 혹은 filter 을 사용해 convoluton 하며 filter는 하나의 weight가 된다. convolution이 끝나면 feature map(filter 가 적용되어 입력 데이터로 detect 된 field 집합) 이 얻어진다.



shared weights 는 filter 가 적용된 결과가 계속 변경되지만 filter 값은 변하지 않는 것을 의미한다. 동일한 weight 가 convolution 할때 동일하게 적용 되는 것을 의미한다.



sub-sampling의 경우 추출한 local feature로 부터 입력된 데이터의 translation distortion에 관계없이 topology 에 영향이 없는 global feature을 추출하기 위함이다. 이 과정을 거쳐 classification이 진행된다.

<div class="card mb-3">
    <img class="card-img-top" src="https://drscdn.500px.org/photo/127767019/q%3D80_m%3D1500/v2?webp=true&sig=dd1fa4580c459472969cd4992068922f311f12cf263cf08b39615cfc1812286b"/>
    <div class="card-body bg-light">
        <div class="card-text">
            The Peak District on a mosty morning.
        </div>
    </div>
</div>
------

LeNet-5는 7개의 layer로 구성된다. 

각각 layer의 train될 파라미터를 계산하는 방법을 단순화 해서 생각하면 다음과 같다.

convolution 된 layer의 parameter 

= (weight*입력map개수 + bias)* feature map 개수



subsampling 된 layer 의 parameter 

= (weight * bias) * feature map 개수



Fully connected layer의 parameter

= (weight * bias) * output 개수

{% endhighlight %}